{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CFM Challenge\n",
        "2018-07. \n",
        "\n",
        "_Note_ : This model gives lower scores than the actual results on the challenge : \n",
        "   - Indeed the notebook will give you around ~21.35 by averaging the 5kfolds which will give around ~20.94 on the public leaderboard. This might be due to high regularisation on the model and also because the test set give better predictions than the train set. \n",
        "\n",
        "To get the best score  on the leaderboard at the end of the competition (around ~20.88), I've just averaged the results of the top 10 best csvs results with different models I've tried during the competion.\n",
        "The different models tried were always pretty the sames. What basically changes is the size of the layers, the concatenation/multiplication/addition in the model, the way you deal with nans in the dataset and what kind of engineered features you will take in input + embeddings size (ok that's a lot of changes... but no matter what you do, you will have more or else the same result with this model).\n"
      ],
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "If you want more details about the model and how does it work, you can find it on the attached slides. This presentation was given at CFM and the end of the academic competition and gives some insights of what worked and what did not.\n",
        "\n",
        "PS : The script isn't really well coded...  basically because I didn't really take the time to do a whole refactoring and it was made of a lot of iterations. So some stuff should not be done like this anymore and doesn't really make sens.. (sorry for that !) \n",
        "\n",
        "If you have any question or suggestions (or if you get way better scores than me !), let me know, I would be happy to discuss and learn ;).\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Let's import some stuff"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential, Model\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import multiply, Layer, LeakyReLU, PReLU, Bidirectional, merge, Dense, GlobalAveragePooling1D, SpatialDropout1D,concatenate,add,Embedding,Input,Masking,LSTM,Flatten,TimeDistributed,GlobalMaxPooling1D,GRU,Activation,Dropout,Reshape, BatchNormalization\n",
        "from sklearn.preprocessing import LabelEncoder, Imputer, StandardScaler, RobustScaler, MinMaxScaler\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau,TensorBoard, LearningRateScheduler\n",
        "from keras import losses\n",
        "from keras import initializers, regularizers, constraints\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.model_selection import KFold,train_test_split\n",
        "import keras\n",
        "import hashlib\n",
        "import math\n",
        "\n\n",
        "#These libs are not from me, I've just copy past the code to use them... \n",
        "#You can find the original repos below and the papers associated in the code.\n",
        "from CLR import CyclicLR #https://github.com/bckenstler/CLR/blob/master/clr_callback.py | https://arxiv.org/abs/1506.01186\n",
        "from janet import JANET #https://github.com/titu1994/Keras-just-another-network-JANET/blob/master/janet.py | https://arxiv.org/abs/1804.04849\n",
        "import gc\n",
        "print(keras.__version__)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/Users/cdelabre/anaconda/lib/python2.7/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
            "  from ._conv import register_converters as _register_converters\n",
            "Using TensorFlow backend.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.1.3\n"
          ]
        }
      ],
      "execution_count": 1,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Get the data"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "home_dir = \"data/\"\n",
        "def mape(y_true, y_pred): \n",
        "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
        "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
        "\n",
        "id_col = \"ID\"\n",
        "target_col = \"TARGET\"\n"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv(\"%s/training_input.csv\" % home_dir, sep=\";\")\n",
        "id_df = pd.read_csv(\"%s/id.csv\" % home_dir, sep=\";\")\n",
        "test_df = pd.read_csv(\"%s/testing_input.csv\" % home_dir, sep=\";\")\n",
        "train_df  = train_df.merge(id_df, on=\"ID\")\n",
        "del id_df; gc.collect()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 3,
          "data": {
            "text/plain": [
              "21"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 3,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "volatility_cols = [c for c in train_df.columns if c.startswith(\"volatility\")]\n",
        "return_cols = [c for c in train_df.columns if c.startswith(\"return\")]\n",
        "other_cols = [\"date\" , \"product_id\"]"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Some feature engineering"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#sum of nans\n",
        "for df in [train_df,test_df]:\n",
        "    df[\"vol_nan\"] = df[volatility_cols].isnull().sum(axis=1)"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#replace nans by interpolation\n",
        "for df in [train_df,test_df]:\n",
        "    for x in [volatility_cols,return_cols]:\n",
        "        df[x] = df[x].interpolate(axis=1, limit_direction=\"both\", inplace=False)"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_mean_groupby(all_data, groupby_cols):\n",
        "    for groupby_obj in groupby_cols:\n",
        "        groupby_col = groupby_obj[\"id\"]\n",
        "        print(groupby_col)\n",
        "        cols = groupby_obj[\"cols\"]\n",
        "        group_by = all_data.groupby([groupby_col])\n",
        "        data_arr = []\n",
        "        data_arr.append({\"i\": \"avg\", \"d\": group_by[cols].mean()})\n",
        "        data_arr.append({\"i\": \"skew\", \"d\": group_by[cols].skew()})\n",
        "        data_arr.append({\"i\": \"kurt\", \"d\": group_by[cols].apply(pd.DataFrame.kurt)})\n",
        "        data_arr.append({\"i\": \"std\", \"d\": group_by[cols].std()})\n",
        "        data_arr.append({\"i\": \"median\", \"d\": group_by[cols].median()})\n",
        "        data_arr.append({\"i\": \"nan\", \"d\": all_data.isnull().groupby(all_data[groupby_col])[cols].sum()})\n",
        "\n",
        "        all_data.set_index([groupby_col], inplace=True)\n",
        "        for obj_data in data_arr:\n",
        "            names = ['%s_%s_%s' % (obj_data[\"i\"], groupby_col, col) for col in cols]            \n",
        "            all_data[names] = (obj_data[\"d\"]).astype(\"float32\")\n",
        "        all_data.reset_index(inplace=True)\n",
        "    return all_data\n",
        "\n",
        "def group_by_date_countd(all_data):\n",
        "    groupby_col = \"date\"\n",
        "    unique_products = all_data.groupby([groupby_col])[\"product_id\"].nunique()\n",
        "    all_data.set_index([groupby_col], inplace=True)\n",
        "    all_data[\"countd_product\"] = unique_products.astype('uint16')\n",
        "    all_data.reset_index(inplace=True)\n",
        "    return all_data\n",
        "\n\n",
        "def group_by_product_countd(all_data):\n",
        "    groupby_col = \"product_id\"\n",
        "    unique_date = all_data.groupby([groupby_col])[\"date\"].nunique()\n",
        "\n",
        "    all_data.set_index([groupby_col], inplace=True)\n",
        "    all_data[\"countd_date\"] = unique_date.astype('uint16')\n",
        "    all_data.reset_index(inplace=True)\n",
        "    return all_data"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#group by date and product_id to get more features\n",
        "calculation_group_by =[\n",
        "    {\"id\":\"date\" ,\n",
        "     \"cols\": volatility_cols + return_cols,\n",
        "    },\n",
        "    {\"id\":\"product_id\" ,\n",
        "     \"cols\": volatility_cols + return_cols,\n",
        "    }\n",
        "]\n",
        "train_df[\"is_train\"] = True\n",
        "test_df[\"is_train\"] = False\n",
        "test_df[\"TARGET\"] = None\n",
        "all_data = pd.concat([train_df, test_df])\n",
        "del train_df\n",
        "del test_df\n",
        "\n",
        "print(\"Creating new aggregated features\")\n",
        "all_data = get_mean_groupby(all_data, calculation_group_by)\n",
        "print(\"Group by date\")\n",
        "all_data = group_by_date_countd(all_data)\n",
        "print(\"Group by product\")\n",
        "all_data = group_by_product_countd(all_data)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new aggregated features\n",
            "date\n",
            "product_id\n",
            "Group by date\n",
            "Group by product\n"
          ]
        }
      ],
      "execution_count": 8,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#replace nan per 0\n",
        "rep_values= 0\n",
        "all_data = all_data.replace([np.inf, -np.inf], rep_values)\n",
        "all_data.fillna(rep_values, inplace=True)"
      ],
      "outputs": [],
      "execution_count": 9,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare to Train"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#get all cols per type\n",
        "#TODO refactor to do it in a for loop \n",
        "volatility_avg_date_cols = [\"avg_date_%s\" % x for x in volatility_cols]\n",
        "volatility_avg_product_cols = [\"avg_product_id_%s\" % x for x in volatility_cols]\n",
        "return_avg_date_cols = [\"avg_date_%s\" % x for x in return_cols]\n",
        "return_avg_product_cols = [\"avg_product_id_%s\" % x for x in return_cols]\n",
        "\n",
        "volatility_std_date_cols = [\"std_date_%s\" % x for x in volatility_cols]\n",
        "volatility_std_product_cols = [\"std_product_id_%s\" % x for x in volatility_cols]\n",
        "return_std_date_cols = [\"std_date_%s\" % x for x in return_cols]\n",
        "return_std_product_cols = [\"std_product_id_%s\" % x for x in return_cols]\n",
        "\n",
        "volatility_median_date_cols = [\"median_date_%s\" % x for x in volatility_cols]\n",
        "volatility_median_product_cols = [\"median_product_id_%s\" % x for x in volatility_cols]\n",
        "\n",
        "volatility_kurt_date_cols = [\"kurt_date_%s\" % x for x in volatility_cols]\n",
        "volatility_kurt_product_cols = [\"kurt_product_id_%s\" % x for x in volatility_cols]\n",
        "return_kurt_date_cols = [\"kurt_date_%s\" % x for x in return_cols]\n",
        "return_kurt_product_cols = [\"kurt_product_id_%s\" % x for x in return_cols]\n",
        "\n",
        "volatility_nan_date_cols = [\"nan_date_%s\" % x for x in volatility_cols]\n",
        "volatility_nan_product_cols = [\"nan_product_id_%s\" % x for x in volatility_cols]"
      ],
      "outputs": [],
      "execution_count": 10,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#encode and preprocess data\n",
        "\n",
        "def fit_encoder(obj_in_cols_obj, all_data, encoder_name=\"StandardScaler\"):\n",
        "    if encoder_name is not None:\n",
        "        for i,x in enumerate(obj_in_cols_obj):\n",
        "            group_cols = x[\"cols\"] #group of lstm cols\n",
        "            encoders = []\n",
        "            for cols in group_cols:\n",
        "                X_all = all_data[cols].values\n",
        "                encoder = df_mapper(cols, X_all, encoder_name)\n",
        "                encoders.append(encoder)\n",
        "            obj_in_cols_obj[i][\"encoders\"] = encoders\n",
        "    return obj_in_cols_obj\n",
        "\n",
        "def df_mapper(cols, all_data , encoder):\n",
        "    if encoder == \"StandardScaler\":\n",
        "        encoder = StandardScaler() \n",
        "    elif encoder == \"LabelEncoder\":\n",
        "        encoder = LabelEncoder()\n",
        "    elif encoder == \"RobustScaler\":\n",
        "        encoder = RobustScaler()\n",
        "    elif encoder == \"MinMaxScaler\":\n",
        "        encoder = MinMaxScaler(feature_range=(0,1))\n",
        "    else:\n",
        "        raise NotFoundException()\n",
        "        \n",
        "    encoder = encoder.fit(all_data)\n",
        "    return encoder\n",
        " \n",
        "def prepare_data(lstm_in_cols_obj, emb_in_cols_obj, df_to_prepare):\n",
        "    def __prepare_data_inn(cols, df_tmp,  encoders, encode=True, reshape=True):\n",
        "        Xs = []\n",
        "        for i, col in enumerate(cols):\n",
        "            X = df_tmp[col].values # timeseries cols\n",
        "            if encode and encoders[i] is not None:\n",
        "                X = encoders[i].transform(X)\n",
        "            if reshape:\n",
        "                X = X.reshape(X.shape[0], X.shape[1], 1)\n",
        "            Xs.append(X)\n",
        "        X = np.concatenate([x for x in Xs] , axis=len(Xs[0].shape)-1) if len(Xs) > 1 else Xs[0] \n",
        "        return X\n",
        "       \n",
        "    Xs_map = []\n",
        "    for x in lstm_in_cols_obj:\n",
        "        cols = x[\"cols\"]\n",
        "        encoders = x[\"encoders\"] if \"encoders\" in x else None\n",
        "        encode = True if \"encoders\" in x else False\n",
        "        Xs_map.append(__prepare_data_inn(cols, df_tmp=df_to_prepare, reshape=True, encoders=encoders, encode=encode))\n",
        "    #TODO Refactor ravel\n",
        "    embs = []\n",
        "    for x in emb_in_cols_obj:\n",
        "        cols = x[\"cols\"]\n",
        "        encoders = x[\"encoders\"] if \"encoders\" in x else None\n",
        "        encode = True if \"encoders\" in x else False\n",
        "        X_emb = __prepare_data_inn(cols, df_tmp=df_to_prepare, reshape=False, encoders=encoders, encode=encode)\n",
        "        embs.append(X_emb)\n",
        "    X_t = [x for x in Xs_map] + embs\n",
        "\n",
        "    return X_t\n",
        "\n\n",
        "def generate(df_generator, target, batch_size, lstm_in_cols_obj,emb_in_cols_obj, y_vals=True, shuffle=True):\n",
        "    imax = int(math.ceil((df_generator.shape[0])/batch_size))\n",
        "    while True:\n",
        "        indexes = __get_exploration_order(df_generator, shuffle)\n",
        "        for i in range(imax):\n",
        "            upp = min((i+1)*batch_size, df_generator.shape[0])\n",
        "            df_tmp = df_generator.loc[indexes[i*batch_size:upp]]\n",
        "            X = prepare_data(lstm_in_cols_obj,emb_in_cols_obj, df_tmp)\n",
        "            if y_vals:\n",
        "                y = df_tmp[target].values\n",
        "                yield X, y\n",
        "            else:\n",
        "                yield X\n",
        "\n",
        "                \n",
        "def __get_exploration_order(df, shuffle=True):\n",
        "    indexes = df.index.values\n",
        "    if shuffle == True:\n",
        "        np.random.shuffle(indexes)\n",
        "    return indexes\n"
      ],
      "outputs": [],
      "execution_count": 11,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#create the model\n",
        "def create_model(lstm_in_cols_obj, emb_in_cols_obj):\n",
        "    def create_emb(name, out_len, in_len):\n",
        "        inp = Input(shape=[1], dtype='int64', name=\"%s_in\" % name)\n",
        "        out = (Embedding(in_len, out_len, name=\"%s_emb\" % name)(inp))\n",
        "        out = SpatialDropout1D(0.2)(out)\n",
        "        out = Flatten()(out)\n",
        "        return inp,out\n",
        "\n\n",
        "    def create_lstm(name, lstm_out, timesteps_in, dims_in):\n",
        "        model_inp = Input(shape=(timesteps_in,dims_in), name=\"%s_in\" % name)\n",
        "        model_out = JANET(lstm_out,return_sequences=False,name=\"%s_lstm\" % name)(model_inp)\n",
        "        return model_inp, model_out\n",
        "\n",
        "    lstm_in_array= []\n",
        "    lstm_out_array = []\n",
        "    for x in lstm_in_cols_obj:\n",
        "        name = x[\"name\"]\n",
        "        dims_in = len(x[\"cols\"])\n",
        "        lstm_out = x[\"out_l\"]\n",
        "        x_model_in, x_model_out = create_lstm(name=name, lstm_out=lstm_out, timesteps_in=54, dims_in=dims_in)\n",
        "        lstm_in_array.append(x_model_in)\n",
        "        lstm_out_array.append(x_model_out)\n",
        "    \n",
        "    #add first two\n",
        "    model_lstm_add_two = add([lstm_out_array[0] , lstm_out_array[1]])\n",
        "    model_lstm_add_two = PReLU()(model_lstm_add_two)\n",
        "    #add last three lstm\n",
        "    model_lstm_add_three = add([lstm_out_array[2] , lstm_out_array[3] , lstm_out_array[4]])\n",
        "    model_lstm_add_three = PReLU()(model_lstm_add_three)\n",
        "\n",
        "    #concat with add layers + origin layer\n",
        "    model_lstm = concatenate([model_lstm_add_three] + [model_lstm_add_two] + lstm_out_array)\n",
        "    model_lstm = (Dense(200)(model_lstm))\n",
        "    model_lstm = PReLU()(model_lstm)\n",
        "\n",
        "    #create embeddings\n",
        "    in_array_emb= []\n",
        "    out_array_emb = []\n",
        "    for x in emb_in_cols_obj:  \n",
        "        in_len = x[\"in_l\"]\n",
        "        out_len = x[\"out_l\"]\n",
        "        name = x[\"name\"]\n",
        "        inp_emb, out_emb = create_emb(name=name , out_len=out_len, in_len = in_len)\n",
        "        in_array_emb.append(inp_emb)\n",
        "        out_array_emb.append((out_emb))\n",
        "        \n",
        "    model_emb = concatenate(out_array_emb) if len(out_array_emb) > 1 else out_array_emb[0]\n",
        "    model_emb = Dropout(0.2)(Dense(100)(model_emb))\n",
        "    model_emb = PReLU()(model_emb)\n",
        "    \n",
        "    #concatenate embeddings & lstms\n",
        "    model = concatenate([model_emb] + [model_lstm]+ [model_lstm_add_two])\n",
        "\n",
        "    model = Dense(300)(model)\n",
        "    model = PReLU()(model)\n",
        "    model = Dropout(0.2)(model)\n",
        "    model = Dense(1, activation=\"linear\")(model)\n",
        "    model =  Model(lstm_in_array+in_array_emb, model)\n",
        "    \n",
        "    return model\n",
        "    "
      ],
      "outputs": [],
      "execution_count": 12,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Params for model"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#params of the model\n",
        "lstm_in_cols_obj = [\n",
        "{\"name\" : \"volatility\" , \"cols\": [volatility_cols] , \"out_l\": 45} ,\n",
        "{\"name\" : \"return\" , \"cols\": [return_cols] , \"out_l\": 45} ,\n",
        "{\"name\" : \"volatility_per_product\" , \"cols\": [volatility_avg_product_cols, volatility_median_product_cols, volatility_std_product_cols, volatility_nan_product_cols], \"out_l\": 15} ,\n",
        "{\"name\" : \"volatility_per_date\" , \"cols\": [volatility_avg_date_cols, volatility_median_date_cols, volatility_std_date_cols, volatility_nan_date_cols], \"out_l\": 15} , \n",
        "{\"name\" : \"return_per_date_and_product\" , \"cols\": [return_avg_date_cols, return_kurt_date_cols , return_avg_product_cols, return_kurt_product_cols ], \"out_l\": 15}\n",
        "]\n",
        "\n",
        "emb_in_cols_obj = [\n",
        "    {\"name\" : \"emb_product_id\" , \"cols\": [[\"product_id\"]] , \"out_l\": 80, \"in_l\" : all_data[\"product_id\"].nunique()} ,\n",
        "    {\"name\" : \"countd_date\" , \"cols\": [[\"countd_date\"]] , \"out_l\": 10 , \"in_l\" : all_data[\"countd_date\"].nunique()} ,\n",
        "    {\"name\" : \"countd_product\" , \"cols\": [[\"countd_product\"]] , \"out_l\": 5 , \"in_l\" : all_data[\"countd_product\"].nunique()} ,\n",
        "    {\"name\" : \"countd_vol_nan\" , \"cols\": [[\"vol_nan\"]] , \"out_l\": 5 , \"in_l\" : all_data[\"vol_nan\"].nunique()}\n",
        "]"
      ],
      "outputs": [],
      "execution_count": 13,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### add encoders in lstm_in_cols_obj \n",
        "lstm_in_cols_obj = fit_encoder(lstm_in_cols_obj, all_data , encoder_name=\"StandardScaler\")\n",
        "emb_in_cols_obj = fit_encoder(emb_in_cols_obj, all_data , encoder_name=\"LabelEncoder\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/Users/cdelabre/anaconda/lib/python2.7/site-packages/sklearn/preprocessing/label.py:95: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        }
      ],
      "execution_count": 14,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "test_df = all_data[all_data[\"is_train\"] == False]\n",
        "train_df = all_data[all_data[\"is_train\"] == True]\n",
        "del all_data;"
      ],
      "outputs": [],
      "execution_count": 15,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = prepare_data(lstm_in_cols_obj, emb_in_cols_obj, test_df)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/Users/cdelabre/anaconda/lib/python2.7/site-packages/sklearn/preprocessing/label.py:128: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        }
      ],
      "execution_count": 16,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#model = create_model(lstm_in_cols_obj, emb_in_cols_obj)\n",
        "#model.summary()"
      ],
      "outputs": [],
      "execution_count": 17,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train the model with 5 KFold "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#sort values in another order\n",
        "#sort values per date in order to NOT train with dates (validation is done with dates never seen like in the test set) \n",
        "seed = 42\n",
        "train_df[\"shuffle\"] = train_df['date'].apply(lambda x: hashlib.sha256(str(x*seed)).hexdigest())\n",
        "train_df.sort_values([\"shuffle\"], inplace=True)\n",
        "monitor = \"val_loss\"\n",
        "model_prefix = \"cfm-example-model-%s\" % datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
        "total_folds = 5\n",
        "total_it = 120 "
      ],
      "outputs": [],
      "execution_count": 18,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kfold = KFold(n_splits=total_folds, shuffle=False)\n",
        "cvscores = []\n",
        "test_preds = []\n",
        "batch_size = (len(train_df) - len(train_df)//total_folds)//(total_it)\n",
        "i = 1\n",
        "for train, valid in kfold.split(train_df):\n",
        "    tmp_df_train = train_df.loc[train].reset_index(drop=True)\n",
        "    tmp_df_valid = train_df.loc[valid].sample(frac=1).reset_index(drop=True)\n",
        "    print(\"Train %d , Valid %d\" % (len(tmp_df_train) , len(tmp_df_valid)))\n",
        "\n",
        "    base_lr = 0.001\n",
        "    j = 1 \n",
        "    last_loss_score = 100 #default init.\n",
        "    model = create_model(lstm_in_cols_obj, emb_in_cols_obj)\n",
        "    once = False\n",
        "    print(\"Start Fold %i - start base_lr %0.4f : \" % (i , base_lr))\n",
        "    while True:\n",
        "        model_checkpoint_name_tmp = \"weights/%s-best-fold-%d-it-%d.h5\" % (model_prefix, i, j )\n",
        "        callbacks = [EarlyStopping(monitor=monitor, patience=3, verbose=1, min_delta=1e-8, mode='min'),\n",
        "                     ModelCheckpoint(model_checkpoint_name_tmp, monitor=monitor, save_best_only=True, save_weights_only=True, verbose=1, mode=\"min\")\n",
        "                    ]\n",
        "        step_size = 100\n",
        "        clr  = CyclicLR(mode='exp_range', max_lr=base_lr*6, base_lr=base_lr, step_size=step_size, logs=False)\n",
        "        callbacks.append(clr)\n",
        "        \n",
        "        train_gen = generate(tmp_df_train, target_col, batch_size, lstm_in_cols_obj, emb_in_cols_obj, True, True)\n",
        "        valid_gen = generate(tmp_df_valid, target_col, batch_size, lstm_in_cols_obj, emb_in_cols_obj, True, False)\n",
        "        \n",
        "        model.compile(loss= \"mean_absolute_percentage_error\",                      \n",
        "                  optimizer=Adam(),\n",
        "                      metrics=[\"mae\"])\n",
        "        hist = model.fit_generator(\n",
        "            generator=train_gen,\n",
        "            validation_data=valid_gen,\n",
        "            steps_per_epoch = math.ceil((tmp_df_train.shape[0])/batch_size),\n",
        "            validation_steps = math.ceil((tmp_df_valid.shape[0])/batch_size),\n",
        "            verbose=1,epochs=50,\n",
        "            callbacks = callbacks,\n",
        "            max_queue_size=30, workers=8\n",
        "                 )         \n",
        "        model.load_weights(model_checkpoint_name_tmp)\n",
        "        loss_score,score = model.evaluate_generator(valid_gen, steps=math.ceil((tmp_df_valid.shape[0])/batch_size))\n",
        "        print(\"Checkpoint. Fold %i | Iteration %i (LR %0.6f) -- Score --> %0.8f | %0.8f \"  % (i, j , base_lr , score,  loss_score))\n",
        "        if loss_score > last_loss_score:\n",
        "            print(\"Score Worse than before (%0.8f | %0.8f)\" % ( loss_score , last_loss_score))\n",
        "            if once:\n",
        "                break\n",
        "            once = True\n",
        "            print(\"Try one last time reducing LR.\")\n",
        "            base_lr = base_lr/10\n",
        "\n",
        "        else:\n",
        "            once = False\n",
        "            print(\"Score Better than before (%0.8f | %0.8f)\" % (loss_score ,last_loss_score))\n",
        "            last_loss_score=loss_score\n",
        "            base_lr = base_lr/5\n",
        "            best_j = j \n",
        "            print(\"Reducing LR to %f\" % base_lr)\n",
        "        if j >3:\n",
        "            print(\"Stop. Too Much Iterations.\")\n",
        "            break\n",
        "        model_checkpoint_name_tmp = \"weights/%s-best-fold-%d-it-%d.h5\" % (model_prefix, i, best_j )\n",
        "        model.load_weights(model_checkpoint_name_tmp)\n",
        "        j = j+1\n",
        "            \n",
        "\n",
        "    print(\"End of Training for Fold %i. Predicting.\"  % (i))\n",
        "    #GET LAST SCORE\n",
        "    model_checkpoint_name = \"weights/%s-best-fold-%d-it-%d.h5\" % (model_prefix, i, best_j )\n",
        "    model.load_weights(model_checkpoint_name)\n",
        "    #save best model\n",
        "    model.save_weights(\"weights/%s-best-fold-%d-Best.h5\" % (model_prefix, i) , overwrite=True )\n",
        "    score,score_mae = model.evaluate_generator(valid_gen, steps=math.ceil(len(tmp_df_valid)/batch_size))\n",
        "\n",
        "    test_yPreds = model.predict(X_test, batch_size=2048, verbose=0)\n",
        "    cvscores.append((score_mae,score))\n",
        "    test_preds.append(test_yPreds[:,-1])\n",
        "    \n",
        "    print(\"End of Fold %i | Score --> %0.4f | %0.4f\"  % (i, score, score_mae) )\n",
        "    i=i+1"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train 509050 , Valid 127263\n",
            "Start Fold 1 - start base_lr 0.0010 : \n",
            "Epoch 1/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 31.1951 - mean_absolute_error: 0.0545\n",
            "Epoch 00001: val_loss improved from inf to 24.00898, saving model to weights/cfm-example-model-2018-07-05-best-fold-1-it-1.h5\n",
            "120/120 [==============================] - 173s 1s/step - loss: 31.1370 - mean_absolute_error: 0.0545 - val_loss: 24.0090 - val_mean_absolute_error: 0.0451\n",
            "Epoch 2/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 22.2450 - mean_absolute_error: 0.0426\n",
            "Epoch 00002: val_loss improved from 24.00898 to 22.33117, saving model to weights/cfm-example-model-2018-07-05-best-fold-1-it-1.h5\n",
            "120/120 [==============================] - 162s 1s/step - loss: 22.2435 - mean_absolute_error: 0.0426 - val_loss: 22.3312 - val_mean_absolute_error: 0.0434\n",
            "Epoch 3/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 21.7590 - mean_absolute_error: 0.0413\n",
            "Epoch 00003: val_loss improved from 22.33117 to 21.80474, saving model to weights/cfm-example-model-2018-07-05-best-fold-1-it-1.h5\n",
            "120/120 [==============================] - 165s 1s/step - loss: 21.7557 - mean_absolute_error: 0.0413 - val_loss: 21.8047 - val_mean_absolute_error: 0.0421\n",
            "Epoch 4/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 21.3885 - mean_absolute_error: 0.0404\n",
            "Epoch 00004: val_loss did not improve\n",
            "120/120 [==============================] - 162s 1s/step - loss: 21.3846 - mean_absolute_error: 0.0404 - val_loss: 22.4653 - val_mean_absolute_error: 0.0430\n",
            "Epoch 5/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 21.3236 - mean_absolute_error: 0.0401\n",
            "Epoch 00005: val_loss improved from 21.80474 to 21.62186, saving model to weights/cfm-example-model-2018-07-05-best-fold-1-it-1.h5\n",
            "120/120 [==============================] - 223s 2s/step - loss: 21.3199 - mean_absolute_error: 0.0401 - val_loss: 21.6219 - val_mean_absolute_error: 0.0416\n",
            "Epoch 6/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 21.1690 - mean_absolute_error: 0.0396\n",
            "Epoch 00006: val_loss did not improve\n",
            "120/120 [==============================] - 254s 2s/step - loss: 21.1655 - mean_absolute_error: 0.0396 - val_loss: 22.2409 - val_mean_absolute_error: 0.0413\n",
            "Epoch 7/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 21.0776 - mean_absolute_error: 0.0394\n",
            "Epoch 00007: val_loss did not improve\n",
            "120/120 [==============================] - 251s 2s/step - loss: 21.0802 - mean_absolute_error: 0.0394 - val_loss: 21.7632 - val_mean_absolute_error: 0.0426\n",
            "Epoch 8/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 21.0007 - mean_absolute_error: 0.0392\n",
            "Epoch 00008: val_loss did not improve\n",
            "120/120 [==============================] - 251s 2s/step - loss: 21.0013 - mean_absolute_error: 0.0392 - val_loss: 22.0499 - val_mean_absolute_error: 0.0415\n",
            "Epoch 00008: early stopping\n",
            "Checkpoint. Fold 1 | Iteration 1 (LR 0.001000) -- Score --> 0.04159507 | 21.62186356 \n",
            "Score Better than before (21.62186356 | 100.00000000)\n",
            "Reducing LR to 0.000200\n",
            "Epoch 1/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 21.0619 - mean_absolute_error: 0.0395\n",
            "Epoch 00001: val_loss improved from inf to 21.93708, saving model to weights/cfm-example-model-2018-07-05-best-fold-1-it-2.h5\n",
            "120/120 [==============================] - 260s 2s/step - loss: 21.0600 - mean_absolute_error: 0.0395 - val_loss: 21.9371 - val_mean_absolute_error: 0.0419\n",
            "Epoch 2/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 20.9018 - mean_absolute_error: 0.0391\n",
            "Epoch 00002: val_loss improved from 21.93708 to 21.90884, saving model to weights/cfm-example-model-2018-07-05-best-fold-1-it-2.h5\n",
            "120/120 [==============================] - 251s 2s/step - loss: 20.9027 - mean_absolute_error: 0.0391 - val_loss: 21.9088 - val_mean_absolute_error: 0.0413\n",
            "Epoch 3/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 20.8388 - mean_absolute_error: 0.0389\n",
            "Epoch 00003: val_loss did not improve\n",
            "120/120 [==============================] - 251s 2s/step - loss: 20.8342 - mean_absolute_error: 0.0389 - val_loss: 22.0786 - val_mean_absolute_error: 0.0418\n",
            "Epoch 4/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 20.6445 - mean_absolute_error: 0.0385\n",
            "Epoch 00004: val_loss did not improve\n",
            "120/120 [==============================] - 254s 2s/step - loss: 20.6434 - mean_absolute_error: 0.0385 - val_loss: 22.0453 - val_mean_absolute_error: 0.0419\n",
            "Epoch 5/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 20.5844 - mean_absolute_error: 0.0383\n",
            "Epoch 00005: val_loss improved from 21.90884 to 21.68674, saving model to weights/cfm-example-model-2018-07-05-best-fold-1-it-2.h5\n",
            "120/120 [==============================] - 252s 2s/step - loss: 20.5808 - mean_absolute_error: 0.0383 - val_loss: 21.6867 - val_mean_absolute_error: 0.0421\n",
            "Epoch 6/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 20.4334 - mean_absolute_error: 0.0379\n",
            "Epoch 00006: val_loss did not improve\n",
            "120/120 [==============================] - 253s 2s/step - loss: 20.4339 - mean_absolute_error: 0.0379 - val_loss: 22.1236 - val_mean_absolute_error: 0.0427\n",
            "Epoch 7/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 20.3008 - mean_absolute_error: 0.0375\n",
            "Epoch 00007: val_loss did not improve\n",
            "120/120 [==============================] - 252s 2s/step - loss: 20.2997 - mean_absolute_error: 0.0375 - val_loss: 22.1713 - val_mean_absolute_error: 0.0418\n",
            "Epoch 8/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 20.2047 - mean_absolute_error: 0.0373\n",
            "Epoch 00008: val_loss did not improve\n",
            "120/120 [==============================] - 252s 2s/step - loss: 20.2042 - mean_absolute_error: 0.0373 - val_loss: 22.0188 - val_mean_absolute_error: 0.0422\n",
            "Epoch 00008: early stopping\n",
            "Checkpoint. Fold 1 | Iteration 2 (LR 0.000200) -- Score --> 0.04206059 | 21.68673751 \n",
            "Score Worse than before (21.68673751 | 21.62186356)\n",
            "Try one last time reducing LR.\n",
            "Epoch 1/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 20.9772 - mean_absolute_error: 0.0395\n",
            "Epoch 00001: val_loss improved from inf to 21.58716, saving model to weights/cfm-example-model-2018-07-05-best-fold-1-it-3.h5\n",
            "120/120 [==============================] - 257s 2s/step - loss: 20.9764 - mean_absolute_error: 0.0394 - val_loss: 21.5872 - val_mean_absolute_error: 0.0416\n",
            "Epoch 2/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 20.9377 - mean_absolute_error: 0.0394\n",
            "Epoch 00002: val_loss did not improve\n",
            "120/120 [==============================] - 251s 2s/step - loss: 20.9371 - mean_absolute_error: 0.0394 - val_loss: 21.6051 - val_mean_absolute_error: 0.0416\n",
            "Epoch 3/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 20.9051 - mean_absolute_error: 0.0393\n",
            "Epoch 00003: val_loss did not improve\n",
            "120/120 [==============================] - 250s 2s/step - loss: 20.9102 - mean_absolute_error: 0.0393 - val_loss: 21.6103 - val_mean_absolute_error: 0.0415\n",
            "Epoch 4/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 20.8853 - mean_absolute_error: 0.0392\n",
            "Epoch 00004: val_loss improved from 21.58716 to 21.57486, saving model to weights/cfm-example-model-2018-07-05-best-fold-1-it-3.h5\n",
            "120/120 [==============================] - 251s 2s/step - loss: 20.8870 - mean_absolute_error: 0.0392 - val_loss: 21.5749 - val_mean_absolute_error: 0.0415\n",
            "Epoch 5/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 20.8598 - mean_absolute_error: 0.0392\n",
            "Epoch 00005: val_loss did not improve\n",
            "120/120 [==============================] - 250s 2s/step - loss: 20.8637 - mean_absolute_error: 0.0392 - val_loss: 21.6583 - val_mean_absolute_error: 0.0415\n",
            "Epoch 6/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 20.8388 - mean_absolute_error: 0.0391\n",
            "Epoch 00006: val_loss did not improve\n",
            "120/120 [==============================] - 251s 2s/step - loss: 20.8406 - mean_absolute_error: 0.0391 - val_loss: 21.6125 - val_mean_absolute_error: 0.0416\n",
            "Epoch 7/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 20.8190 - mean_absolute_error: 0.0390\n",
            "Epoch 00007: val_loss did not improve\n",
            "120/120 [==============================] - 250s 2s/step - loss: 20.8170 - mean_absolute_error: 0.0390 - val_loss: 21.5816 - val_mean_absolute_error: 0.0416\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 00007: early stopping\n",
            "Checkpoint. Fold 1 | Iteration 3 (LR 0.000020) -- Score --> 0.04151054 | 21.57486013 \n",
            "Score Better than before (21.57486013 | 21.62186356)\n",
            "Reducing LR to 0.000004\n",
            "Epoch 1/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 20.8356 - mean_absolute_error: 0.0392\n",
            "Epoch 00001: val_loss improved from inf to 21.59430, saving model to weights/cfm-example-model-2018-07-05-best-fold-1-it-4.h5\n",
            "120/120 [==============================] - 255s 2s/step - loss: 20.8330 - mean_absolute_error: 0.0392 - val_loss: 21.5943 - val_mean_absolute_error: 0.0417\n",
            "Epoch 2/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 20.8432 - mean_absolute_error: 0.0392\n",
            "Epoch 00002: val_loss improved from 21.59430 to 21.59142, saving model to weights/cfm-example-model-2018-07-05-best-fold-1-it-4.h5\n",
            "120/120 [==============================] - 250s 2s/step - loss: 20.8442 - mean_absolute_error: 0.0392 - val_loss: 21.5914 - val_mean_absolute_error: 0.0416\n",
            "Epoch 3/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 20.8397 - mean_absolute_error: 0.0391\n",
            "Epoch 00003: val_loss did not improve\n",
            "120/120 [==============================] - 253s 2s/step - loss: 20.8395 - mean_absolute_error: 0.0391 - val_loss: 21.6219 - val_mean_absolute_error: 0.0415\n",
            "Epoch 4/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 20.8221 - mean_absolute_error: 0.0391\n",
            "Epoch 00004: val_loss did not improve\n",
            "120/120 [==============================] - 253s 2s/step - loss: 20.8190 - mean_absolute_error: 0.0391 - val_loss: 21.6053 - val_mean_absolute_error: 0.0415\n",
            "Epoch 5/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 20.8328 - mean_absolute_error: 0.0391\n",
            "Epoch 00005: val_loss did not improve\n",
            "120/120 [==============================] - 250s 2s/step - loss: 20.8301 - mean_absolute_error: 0.0391 - val_loss: 21.6039 - val_mean_absolute_error: 0.0416\n",
            "Epoch 00005: early stopping\n",
            "Checkpoint. Fold 1 | Iteration 4 (LR 0.000004) -- Score --> 0.04156951 | 21.59142106 \n",
            "Score Worse than before (21.59142106 | 21.57486013)\n",
            "Try one last time reducing LR.\n",
            "Stop. Too Much Iterations.\n",
            "End of Training for Fold 1. Predicting.\n",
            "End of Fold 1 | Score --> 21.5749 | 0.0415\n",
            "Train 509050 , Valid 127263\n",
            "Start Fold 2 - start base_lr 0.0010 : \n",
            "Epoch 1/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 31.6170 - mean_absolute_error: 0.0554\n",
            "Epoch 00001: val_loss improved from inf to 22.33339, saving model to weights/cfm-example-model-2018-07-05-best-fold-2-it-1.h5\n",
            "120/120 [==============================] - 251s 2s/step - loss: 31.5448 - mean_absolute_error: 0.0553 - val_loss: 22.3334 - val_mean_absolute_error: 0.0417\n",
            "Epoch 2/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 22.3585 - mean_absolute_error: 0.0432\n",
            "Epoch 00002: val_loss improved from 22.33339 to 21.57576, saving model to weights/cfm-example-model-2018-07-05-best-fold-2-it-1.h5\n",
            "120/120 [==============================] - 249s 2s/step - loss: 22.3539 - mean_absolute_error: 0.0432 - val_loss: 21.5758 - val_mean_absolute_error: 0.0385\n",
            "Epoch 3/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 21.9080 - mean_absolute_error: 0.0421\n",
            "Epoch 00003: val_loss did not improve\n",
            "120/120 [==============================] - 248s 2s/step - loss: 21.9770 - mean_absolute_error: 0.0421 - val_loss: 21.6193 - val_mean_absolute_error: 0.0395\n",
            "Epoch 4/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 21.7433 - mean_absolute_error: 0.0416\n",
            "Epoch 00004: val_loss did not improve\n",
            "120/120 [==============================] - 258s 2s/step - loss: 21.7495 - mean_absolute_error: 0.0416 - val_loss: 21.9674 - val_mean_absolute_error: 0.0388\n",
            "Epoch 5/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 21.7392 - mean_absolute_error: 0.0412\n",
            "Epoch 00005: val_loss improved from 21.57576 to 21.31353, saving model to weights/cfm-example-model-2018-07-05-best-fold-2-it-1.h5\n",
            "120/120 [==============================] - 250s 2s/step - loss: 21.7346 - mean_absolute_error: 0.0412 - val_loss: 21.3135 - val_mean_absolute_error: 0.0383\n",
            "Epoch 6/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 21.6167 - mean_absolute_error: 0.0409\n",
            "Epoch 00006: val_loss did not improve\n",
            "120/120 [==============================] - 257s 2s/step - loss: 21.6149 - mean_absolute_error: 0.0409 - val_loss: 21.4765 - val_mean_absolute_error: 0.0382\n",
            "Epoch 7/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 21.4674 - mean_absolute_error: 0.0406\n",
            "Epoch 00007: val_loss did not improve\n",
            "120/120 [==============================] - 248s 2s/step - loss: 21.4644 - mean_absolute_error: 0.0406 - val_loss: 21.3208 - val_mean_absolute_error: 0.0376\n",
            "Epoch 8/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 21.4656 - mean_absolute_error: 0.0405\n",
            "Epoch 00008: val_loss did not improve\n",
            "120/120 [==============================] - 250s 2s/step - loss: 21.4633 - mean_absolute_error: 0.0405 - val_loss: 21.5292 - val_mean_absolute_error: 0.0395\n",
            "Epoch 00008: early stopping\n",
            "Checkpoint. Fold 2 | Iteration 1 (LR 0.001000) -- Score --> 0.03829290 | 21.31352615 \n",
            "Score Better than before (21.31352615 | 100.00000000)\n",
            "Reducing LR to 0.000200\n",
            "Epoch 1/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 21.3495 - mean_absolute_error: 0.0406\n",
            "Epoch 00001: val_loss improved from inf to 21.56050, saving model to weights/cfm-example-model-2018-07-05-best-fold-2-it-2.h5\n",
            "120/120 [==============================] - 255s 2s/step - loss: 21.3459 - mean_absolute_error: 0.0406 - val_loss: 21.5605 - val_mean_absolute_error: 0.0373\n",
            "Epoch 2/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 21.0738 - mean_absolute_error: 0.0401\n",
            "Epoch 00002: val_loss improved from 21.56050 to 21.26893, saving model to weights/cfm-example-model-2018-07-05-best-fold-2-it-2.h5\n",
            "120/120 [==============================] - 249s 2s/step - loss: 21.0729 - mean_absolute_error: 0.0401 - val_loss: 21.2689 - val_mean_absolute_error: 0.0376\n",
            "Epoch 3/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 21.0831 - mean_absolute_error: 0.0399\n",
            "Epoch 00003: val_loss did not improve\n",
            "120/120 [==============================] - 250s 2s/step - loss: 21.0795 - mean_absolute_error: 0.0399 - val_loss: 21.2795 - val_mean_absolute_error: 0.0378\n",
            "Epoch 4/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 20.8471 - mean_absolute_error: 0.0393\n",
            "Epoch 00004: val_loss improved from 21.26893 to 21.26547, saving model to weights/cfm-example-model-2018-07-05-best-fold-2-it-2.h5\n",
            "120/120 [==============================] - 254s 2s/step - loss: 20.8441 - mean_absolute_error: 0.0394 - val_loss: 21.2655 - val_mean_absolute_error: 0.0379\n",
            "Epoch 5/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 20.8757 - mean_absolute_error: 0.0392\n",
            "Epoch 00005: val_loss did not improve\n",
            "120/120 [==============================] - 249s 2s/step - loss: 20.8807 - mean_absolute_error: 0.0392 - val_loss: 21.3304 - val_mean_absolute_error: 0.0380\n",
            "Epoch 6/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 20.7580 - mean_absolute_error: 0.0389\n",
            "Epoch 00006: val_loss did not improve\n",
            "120/120 [==============================] - 248s 2s/step - loss: 20.7591 - mean_absolute_error: 0.0389 - val_loss: 21.5022 - val_mean_absolute_error: 0.0392\n",
            "Epoch 7/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 20.6218 - mean_absolute_error: 0.0385\n",
            "Epoch 00007: val_loss did not improve\n",
            "120/120 [==============================] - 247s 2s/step - loss: 20.6204 - mean_absolute_error: 0.0385 - val_loss: 21.6113 - val_mean_absolute_error: 0.0377\n",
            "Epoch 00007: early stopping\n",
            "Checkpoint. Fold 2 | Iteration 2 (LR 0.000200) -- Score --> 0.03790891 | 21.26547432 \n",
            "Score Better than before (21.26547432 | 21.31352615)\n",
            "Reducing LR to 0.000040\n",
            "Epoch 1/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 20.7244 - mean_absolute_error: 0.0390\n",
            "Epoch 00001: val_loss improved from inf to 21.30473, saving model to weights/cfm-example-model-2018-07-05-best-fold-2-it-3.h5\n",
            "120/120 [==============================] - 255s 2s/step - loss: 20.7257 - mean_absolute_error: 0.0390 - val_loss: 21.3047 - val_mean_absolute_error: 0.0377\n",
            "Epoch 2/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "119/120 [============================>.] - ETA: 1s - loss: 20.6994 - mean_absolute_error: 0.0389\n",
            "Epoch 00002: val_loss improved from 21.30473 to 21.27991, saving model to weights/cfm-example-model-2018-07-05-best-fold-2-it-3.h5\n",
            "120/120 [==============================] - 250s 2s/step - loss: 20.6971 - mean_absolute_error: 0.0389 - val_loss: 21.2799 - val_mean_absolute_error: 0.0377\n",
            "Epoch 3/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 20.6622 - mean_absolute_error: 0.0388\n",
            "Epoch 00003: val_loss improved from 21.27991 to 21.27225, saving model to weights/cfm-example-model-2018-07-05-best-fold-2-it-3.h5\n",
            "120/120 [==============================] - 249s 2s/step - loss: 20.6604 - mean_absolute_error: 0.0388 - val_loss: 21.2723 - val_mean_absolute_error: 0.0379\n",
            "Epoch 4/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 20.5907 - mean_absolute_error: 0.0387\n",
            "Epoch 00004: val_loss improved from 21.27225 to 21.26652, saving model to weights/cfm-example-model-2018-07-05-best-fold-2-it-3.h5\n",
            "120/120 [==============================] - 249s 2s/step - loss: 20.5915 - mean_absolute_error: 0.0387 - val_loss: 21.2665 - val_mean_absolute_error: 0.0378\n",
            "Epoch 5/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 20.6557 - mean_absolute_error: 0.0386\n",
            "Epoch 00005: val_loss did not improve\n",
            "120/120 [==============================] - 251s 2s/step - loss: 20.6552 - mean_absolute_error: 0.0386 - val_loss: 21.2877 - val_mean_absolute_error: 0.0377\n",
            "Epoch 6/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 20.5383 - mean_absolute_error: 0.0386\n",
            "Epoch 00006: val_loss did not improve\n",
            "120/120 [==============================] - 251s 2s/step - loss: 20.5384 - mean_absolute_error: 0.0385 - val_loss: 21.3753 - val_mean_absolute_error: 0.0376\n",
            "Epoch 7/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 20.5016 - mean_absolute_error: 0.0384\n",
            "Epoch 00007: val_loss did not improve\n",
            "120/120 [==============================] - 253s 2s/step - loss: 20.4991 - mean_absolute_error: 0.0384 - val_loss: 21.2914 - val_mean_absolute_error: 0.0379\n",
            "Epoch 00007: early stopping\n",
            "Checkpoint. Fold 2 | Iteration 3 (LR 0.000040) -- Score --> 0.03777908 | 21.26652095 \n",
            "Score Worse than before (21.26652095 | 21.26547432)\n",
            "Try one last time reducing LR.\n",
            "Epoch 1/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 20.8900 - mean_absolute_error: 0.0390\n",
            "Epoch 00001: val_loss improved from inf to 21.27761, saving model to weights/cfm-example-model-2018-07-05-best-fold-2-it-4.h5\n",
            "120/120 [==============================] - 256s 2s/step - loss: 20.8886 - mean_absolute_error: 0.0391 - val_loss: 21.2776 - val_mean_absolute_error: 0.0377\n",
            "Epoch 2/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 20.8426 - mean_absolute_error: 0.0390\n",
            "Epoch 00002: val_loss improved from 21.27761 to 21.26666, saving model to weights/cfm-example-model-2018-07-05-best-fold-2-it-4.h5\n",
            "120/120 [==============================] - 250s 2s/step - loss: 20.8386 - mean_absolute_error: 0.0390 - val_loss: 21.2667 - val_mean_absolute_error: 0.0377\n",
            "Epoch 3/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 20.6600 - mean_absolute_error: 0.0390\n",
            "Epoch 00003: val_loss improved from 21.26666 to 21.26555, saving model to weights/cfm-example-model-2018-07-05-best-fold-2-it-4.h5\n",
            "120/120 [==============================] - 250s 2s/step - loss: 20.6574 - mean_absolute_error: 0.0390 - val_loss: 21.2655 - val_mean_absolute_error: 0.0377\n",
            "Epoch 4/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 20.7864 - mean_absolute_error: 0.0389\n",
            "Epoch 00004: val_loss improved from 21.26555 to 21.25496, saving model to weights/cfm-example-model-2018-07-05-best-fold-2-it-4.h5\n",
            "120/120 [==============================] - 253s 2s/step - loss: 20.7886 - mean_absolute_error: 0.0389 - val_loss: 21.2550 - val_mean_absolute_error: 0.0377\n",
            "Epoch 5/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 20.6992 - mean_absolute_error: 0.0389\n",
            "Epoch 00005: val_loss did not improve\n",
            "120/120 [==============================] - 249s 2s/step - loss: 20.6949 - mean_absolute_error: 0.0389 - val_loss: 21.2623 - val_mean_absolute_error: 0.0376\n",
            "Epoch 6/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 20.7977 - mean_absolute_error: 0.0389\n",
            "Epoch 00006: val_loss improved from 21.25496 to 21.25376, saving model to weights/cfm-example-model-2018-07-05-best-fold-2-it-4.h5\n",
            "120/120 [==============================] - 250s 2s/step - loss: 20.7917 - mean_absolute_error: 0.0389 - val_loss: 21.2538 - val_mean_absolute_error: 0.0377\n",
            "Epoch 7/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 20.7001 - mean_absolute_error: 0.0389\n",
            "Epoch 00007: val_loss did not improve\n",
            "120/120 [==============================] - 249s 2s/step - loss: 20.7009 - mean_absolute_error: 0.0389 - val_loss: 21.2560 - val_mean_absolute_error: 0.0376\n",
            "Epoch 8/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 20.6726 - mean_absolute_error: 0.0389\n",
            "Epoch 00008: val_loss improved from 21.25376 to 21.24723, saving model to weights/cfm-example-model-2018-07-05-best-fold-2-it-4.h5\n",
            "120/120 [==============================] - 248s 2s/step - loss: 20.6773 - mean_absolute_error: 0.0389 - val_loss: 21.2472 - val_mean_absolute_error: 0.0377\n",
            "Epoch 9/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 20.6298 - mean_absolute_error: 0.0389\n",
            "Epoch 00009: val_loss did not improve\n",
            "120/120 [==============================] - 249s 2s/step - loss: 20.6286 - mean_absolute_error: 0.0389 - val_loss: 21.2576 - val_mean_absolute_error: 0.0376\n",
            "Epoch 10/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 20.6661 - mean_absolute_error: 0.0389\n",
            "Epoch 00010: val_loss did not improve\n",
            "120/120 [==============================] - 249s 2s/step - loss: 20.6647 - mean_absolute_error: 0.0389 - val_loss: 21.2565 - val_mean_absolute_error: 0.0377\n",
            "Epoch 11/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 20.7002 - mean_absolute_error: 0.0389\n",
            "Epoch 00011: val_loss did not improve\n",
            "120/120 [==============================] - 249s 2s/step - loss: 20.6962 - mean_absolute_error: 0.0389 - val_loss: 21.2552 - val_mean_absolute_error: 0.0377\n",
            "Epoch 00011: early stopping\n",
            "Checkpoint. Fold 2 | Iteration 4 (LR 0.000004) -- Score --> 0.03769590 | 21.24722691 \n",
            "Score Better than before (21.24722691 | 21.26547432)\n",
            "Reducing LR to 0.000001\n",
            "Stop. Too Much Iterations.\n",
            "End of Training for Fold 2. Predicting.\n",
            "End of Fold 2 | Score --> 21.2472 | 0.0377\n",
            "Train 509050 , Valid 127263\n",
            "Start Fold 3 - start base_lr 0.0010 : \n",
            "Epoch 1/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 32.7878 - mean_absolute_error: 0.0560\n",
            "Epoch 00001: val_loss improved from inf to 22.82465, saving model to weights/cfm-example-model-2018-07-05-best-fold-3-it-1.h5\n",
            "120/120 [==============================] - 254s 2s/step - loss: 32.7108 - mean_absolute_error: 0.0559 - val_loss: 22.8247 - val_mean_absolute_error: 0.0412\n",
            "Epoch 2/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 22.4199 - mean_absolute_error: 0.0426\n",
            "Epoch 00002: val_loss improved from 22.82465 to 22.10771, saving model to weights/cfm-example-model-2018-07-05-best-fold-3-it-1.h5\n",
            "120/120 [==============================] - 250s 2s/step - loss: 22.4247 - mean_absolute_error: 0.0426 - val_loss: 22.1077 - val_mean_absolute_error: 0.0405\n",
            "Epoch 3/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 22.4206 - mean_absolute_error: 0.0421\n",
            "Epoch 00003: val_loss improved from 22.10771 to 21.55393, saving model to weights/cfm-example-model-2018-07-05-best-fold-3-it-1.h5\n",
            "120/120 [==============================] - 253s 2s/step - loss: 22.4179 - mean_absolute_error: 0.0421 - val_loss: 21.5539 - val_mean_absolute_error: 0.0403\n",
            "Epoch 4/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 21.7516 - mean_absolute_error: 0.0410\n",
            "Epoch 00004: val_loss improved from 21.55393 to 21.09642, saving model to weights/cfm-example-model-2018-07-05-best-fold-3-it-1.h5\n",
            "120/120 [==============================] - 252s 2s/step - loss: 21.7503 - mean_absolute_error: 0.0410 - val_loss: 21.0964 - val_mean_absolute_error: 0.0400\n",
            "Epoch 5/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "119/120 [============================>.] - ETA: 1s - loss: 21.4926 - mean_absolute_error: 0.0405\n",
            "Epoch 00005: val_loss did not improve\n",
            "120/120 [==============================] - 254s 2s/step - loss: 21.4936 - mean_absolute_error: 0.0405 - val_loss: 21.2586 - val_mean_absolute_error: 0.0407\n",
            "Epoch 6/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 21.4877 - mean_absolute_error: 0.0403\n",
            "Epoch 00006: val_loss did not improve\n",
            "120/120 [==============================] - 258s 2s/step - loss: 21.4998 - mean_absolute_error: 0.0403 - val_loss: 22.9731 - val_mean_absolute_error: 0.0414\n",
            "Epoch 7/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 21.6222 - mean_absolute_error: 0.0403\n",
            "Epoch 00007: val_loss did not improve\n",
            "120/120 [==============================] - 254s 2s/step - loss: 21.6208 - mean_absolute_error: 0.0403 - val_loss: 21.2310 - val_mean_absolute_error: 0.0395\n",
            "Epoch 00007: early stopping\n",
            "Checkpoint. Fold 3 | Iteration 1 (LR 0.001000) -- Score --> 0.03998899 | 21.09642111 \n",
            "Score Better than before (21.09642111 | 100.00000000)\n",
            "Reducing LR to 0.000200\n",
            "Epoch 1/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 21.4850 - mean_absolute_error: 0.0405\n",
            "Epoch 00001: val_loss improved from inf to 21.05011, saving model to weights/cfm-example-model-2018-07-05-best-fold-3-it-2.h5\n",
            "120/120 [==============================] - 255s 2s/step - loss: 21.4809 - mean_absolute_error: 0.0405 - val_loss: 21.0501 - val_mean_absolute_error: 0.0399\n",
            "Epoch 2/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 21.2301 - mean_absolute_error: 0.0399\n",
            "Epoch 00002: val_loss improved from 21.05011 to 20.97744, saving model to weights/cfm-example-model-2018-07-05-best-fold-3-it-2.h5\n",
            "120/120 [==============================] - 250s 2s/step - loss: 21.2274 - mean_absolute_error: 0.0398 - val_loss: 20.9774 - val_mean_absolute_error: 0.0396\n",
            "Epoch 3/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 21.1806 - mean_absolute_error: 0.0396\n",
            "Epoch 00003: val_loss improved from 20.97744 to 20.95354, saving model to weights/cfm-example-model-2018-07-05-best-fold-3-it-2.h5\n",
            "120/120 [==============================] - 250s 2s/step - loss: 21.1808 - mean_absolute_error: 0.0396 - val_loss: 20.9535 - val_mean_absolute_error: 0.0399\n",
            "Epoch 4/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 21.0053 - mean_absolute_error: 0.0392\n",
            "Epoch 00004: val_loss did not improve\n",
            "120/120 [==============================] - 249s 2s/step - loss: 21.0060 - mean_absolute_error: 0.0392 - val_loss: 21.0527 - val_mean_absolute_error: 0.0393\n",
            "Epoch 5/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 21.0471 - mean_absolute_error: 0.0390\n",
            "Epoch 00005: val_loss did not improve\n",
            "120/120 [==============================] - 250s 2s/step - loss: 21.0466 - mean_absolute_error: 0.0390 - val_loss: 21.0032 - val_mean_absolute_error: 0.0394\n",
            "Epoch 6/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 20.8825 - mean_absolute_error: 0.0386\n",
            "Epoch 00006: val_loss did not improve\n",
            "120/120 [==============================] - 249s 2s/step - loss: 20.8879 - mean_absolute_error: 0.0387 - val_loss: 21.6537 - val_mean_absolute_error: 0.0397\n",
            "Epoch 00006: early stopping\n",
            "Checkpoint. Fold 3 | Iteration 2 (LR 0.000200) -- Score --> 0.03990571 | 20.95354385 \n",
            "Score Better than before (20.95354385 | 21.09642111)\n",
            "Reducing LR to 0.000040\n",
            "Epoch 1/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 20.9718 - mean_absolute_error: 0.0391\n",
            "Epoch 00001: val_loss improved from inf to 20.95232, saving model to weights/cfm-example-model-2018-07-05-best-fold-3-it-3.h5\n",
            "120/120 [==============================] - 257s 2s/step - loss: 20.9795 - mean_absolute_error: 0.0391 - val_loss: 20.9523 - val_mean_absolute_error: 0.0397\n",
            "Epoch 2/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 20.8778 - mean_absolute_error: 0.0389\n",
            "Epoch 00002: val_loss did not improve\n",
            "120/120 [==============================] - 253s 2s/step - loss: 20.8810 - mean_absolute_error: 0.0389 - val_loss: 20.9601 - val_mean_absolute_error: 0.0394\n",
            "Epoch 3/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 20.7963 - mean_absolute_error: 0.0388\n",
            "Epoch 00003: val_loss did not improve\n",
            "120/120 [==============================] - 254s 2s/step - loss: 20.7941 - mean_absolute_error: 0.0388 - val_loss: 20.9738 - val_mean_absolute_error: 0.0396\n",
            "Epoch 4/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 20.7463 - mean_absolute_error: 0.0387\n",
            "Epoch 00004: val_loss did not improve\n",
            "120/120 [==============================] - 250s 2s/step - loss: 20.7481 - mean_absolute_error: 0.0387 - val_loss: 20.9874 - val_mean_absolute_error: 0.0398\n",
            "Epoch 00004: early stopping\n",
            "Checkpoint. Fold 3 | Iteration 3 (LR 0.000040) -- Score --> 0.03967747 | 20.95232054 \n",
            "Score Better than before (20.95232054 | 20.95354385)\n",
            "Reducing LR to 0.000008\n",
            "Epoch 1/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 20.8422 - mean_absolute_error: 0.0389\n",
            "Epoch 00001: val_loss improved from inf to 20.94621, saving model to weights/cfm-example-model-2018-07-05-best-fold-3-it-4.h5\n",
            "120/120 [==============================] - 253s 2s/step - loss: 20.8468 - mean_absolute_error: 0.0389 - val_loss: 20.9462 - val_mean_absolute_error: 0.0395\n",
            "Epoch 2/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 20.8297 - mean_absolute_error: 0.0389\n",
            "Epoch 00002: val_loss improved from 20.94621 to 20.94402, saving model to weights/cfm-example-model-2018-07-05-best-fold-3-it-4.h5\n",
            "120/120 [==============================] - 249s 2s/step - loss: 20.8308 - mean_absolute_error: 0.0389 - val_loss: 20.9440 - val_mean_absolute_error: 0.0393\n",
            "Epoch 3/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 20.8458 - mean_absolute_error: 0.0389\n",
            "Epoch 00003: val_loss improved from 20.94402 to 20.94247, saving model to weights/cfm-example-model-2018-07-05-best-fold-3-it-4.h5\n",
            "120/120 [==============================] - 251s 2s/step - loss: 20.8490 - mean_absolute_error: 0.0389 - val_loss: 20.9425 - val_mean_absolute_error: 0.0394\n",
            "Epoch 4/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 20.7882 - mean_absolute_error: 0.0388\n",
            "Epoch 00004: val_loss improved from 20.94247 to 20.93321, saving model to weights/cfm-example-model-2018-07-05-best-fold-3-it-4.h5\n",
            "120/120 [==============================] - 251s 2s/step - loss: 20.7921 - mean_absolute_error: 0.0388 - val_loss: 20.9332 - val_mean_absolute_error: 0.0394\n",
            "Epoch 5/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 20.8104 - mean_absolute_error: 0.0388\n",
            "Epoch 00005: val_loss did not improve\n",
            "120/120 [==============================] - 249s 2s/step - loss: 20.8097 - mean_absolute_error: 0.0388 - val_loss: 20.9531 - val_mean_absolute_error: 0.0395\n",
            "Epoch 6/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 20.7543 - mean_absolute_error: 0.0388\n",
            "Epoch 00006: val_loss did not improve\n",
            "120/120 [==============================] - 248s 2s/step - loss: 20.7657 - mean_absolute_error: 0.0388 - val_loss: 20.9462 - val_mean_absolute_error: 0.0395\n",
            "Epoch 7/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 20.7766 - mean_absolute_error: 0.0387\n",
            "Epoch 00007: val_loss did not improve\n",
            "120/120 [==============================] - 249s 2s/step - loss: 20.7759 - mean_absolute_error: 0.0387 - val_loss: 20.9485 - val_mean_absolute_error: 0.0395\n",
            "Epoch 00007: early stopping\n",
            "Checkpoint. Fold 3 | Iteration 4 (LR 0.000008) -- Score --> 0.03940942 | 20.93321025 \n",
            "Score Better than before (20.93321025 | 20.95232054)\n",
            "Reducing LR to 0.000002\n",
            "Stop. Too Much Iterations.\n",
            "End of Training for Fold 3. Predicting.\n",
            "End of Fold 3 | Score --> 20.9332 | 0.0394\n",
            "Train 509051 , Valid 127262\n",
            "Start Fold 4 - start base_lr 0.0010 : \n",
            "Epoch 1/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 31.0696 - mean_absolute_error: 0.0545\n",
            "Epoch 00001: val_loss improved from inf to 23.08213, saving model to weights/cfm-example-model-2018-07-05-best-fold-4-it-1.h5\n",
            "120/120 [==============================] - 257s 2s/step - loss: 31.0097 - mean_absolute_error: 0.0544 - val_loss: 23.0821 - val_mean_absolute_error: 0.0453\n",
            "Epoch 2/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "119/120 [============================>.] - ETA: 1s - loss: 22.3086 - mean_absolute_error: 0.0419\n",
            "Epoch 00002: val_loss improved from 23.08213 to 22.03694, saving model to weights/cfm-example-model-2018-07-05-best-fold-4-it-1.h5\n",
            "120/120 [==============================] - 248s 2s/step - loss: 22.3018 - mean_absolute_error: 0.0419 - val_loss: 22.0369 - val_mean_absolute_error: 0.0444\n",
            "Epoch 3/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 21.9641 - mean_absolute_error: 0.0409\n",
            "Epoch 00003: val_loss did not improve\n",
            "120/120 [==============================] - 250s 2s/step - loss: 21.9567 - mean_absolute_error: 0.0409 - val_loss: 22.3551 - val_mean_absolute_error: 0.0471\n",
            "Epoch 4/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 21.5951 - mean_absolute_error: 0.0401\n",
            "Epoch 00004: val_loss did not improve\n",
            "120/120 [==============================] - 250s 2s/step - loss: 21.5992 - mean_absolute_error: 0.0401 - val_loss: 22.3890 - val_mean_absolute_error: 0.0455\n",
            "Epoch 5/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 21.9985 - mean_absolute_error: 0.0405\n",
            "Epoch 00005: val_loss improved from 22.03694 to 21.67569, saving model to weights/cfm-example-model-2018-07-05-best-fold-4-it-1.h5\n",
            "120/120 [==============================] - 249s 2s/step - loss: 21.9925 - mean_absolute_error: 0.0405 - val_loss: 21.6757 - val_mean_absolute_error: 0.0433\n",
            "Epoch 6/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 21.6486 - mean_absolute_error: 0.0401\n",
            "Epoch 00006: val_loss did not improve\n",
            "120/120 [==============================] - 250s 2s/step - loss: 21.6645 - mean_absolute_error: 0.0401 - val_loss: 22.6773 - val_mean_absolute_error: 0.0438\n",
            "Epoch 7/50\n",
            "119/120 [============================>.] - ETA: 2s - loss: 21.6690 - mean_absolute_error: 0.0398\n",
            "Epoch 00007: val_loss did not improve\n",
            "120/120 [==============================] - 268s 2s/step - loss: 21.6629 - mean_absolute_error: 0.0398 - val_loss: 21.6885 - val_mean_absolute_error: 0.0438\n",
            "Epoch 8/50\n",
            "119/120 [============================>.] - ETA: 2s - loss: 21.6217 - mean_absolute_error: 0.0399\n",
            "Epoch 00008: val_loss did not improve\n",
            "120/120 [==============================] - 271s 2s/step - loss: 21.6192 - mean_absolute_error: 0.0399 - val_loss: 21.7238 - val_mean_absolute_error: 0.0435\n",
            "Epoch 00008: early stopping\n",
            "Checkpoint. Fold 4 | Iteration 1 (LR 0.001000) -- Score --> 0.04326281 | 21.67569453 \n",
            "Score Better than before (21.67569453 | 100.00000000)\n",
            "Reducing LR to 0.000200\n",
            "Epoch 1/50\n",
            "119/120 [============================>.] - ETA: 2s - loss: 21.2552 - mean_absolute_error: 0.0395\n",
            "Epoch 00001: val_loss improved from inf to 21.70666, saving model to weights/cfm-example-model-2018-07-05-best-fold-4-it-2.h5\n",
            "120/120 [==============================] - 283s 2s/step - loss: 21.2513 - mean_absolute_error: 0.0395 - val_loss: 21.7067 - val_mean_absolute_error: 0.0437\n",
            "Epoch 2/50\n",
            "119/120 [============================>.] - ETA: 2s - loss: 21.0308 - mean_absolute_error: 0.0389\n",
            "Epoch 00002: val_loss improved from 21.70666 to 21.69983, saving model to weights/cfm-example-model-2018-07-05-best-fold-4-it-2.h5\n",
            "120/120 [==============================] - 276s 2s/step - loss: 21.0310 - mean_absolute_error: 0.0389 - val_loss: 21.6998 - val_mean_absolute_error: 0.0437\n",
            "Epoch 3/50\n",
            "119/120 [============================>.] - ETA: 2s - loss: 21.0856 - mean_absolute_error: 0.0388\n",
            "Epoch 00003: val_loss improved from 21.69983 to 21.57375, saving model to weights/cfm-example-model-2018-07-05-best-fold-4-it-2.h5\n",
            "120/120 [==============================] - 276s 2s/step - loss: 21.0870 - mean_absolute_error: 0.0388 - val_loss: 21.5737 - val_mean_absolute_error: 0.0429\n",
            "Epoch 4/50\n",
            "119/120 [============================>.] - ETA: 2s - loss: 20.9153 - mean_absolute_error: 0.0384\n",
            "Epoch 00004: val_loss did not improve\n",
            "120/120 [==============================] - 275s 2s/step - loss: 20.9178 - mean_absolute_error: 0.0384 - val_loss: 21.7794 - val_mean_absolute_error: 0.0424\n",
            "Epoch 5/50\n",
            "119/120 [============================>.] - ETA: 2s - loss: 20.8395 - mean_absolute_error: 0.0382\n",
            "Epoch 00005: val_loss did not improve\n",
            "120/120 [==============================] - 270s 2s/step - loss: 20.8366 - mean_absolute_error: 0.0383 - val_loss: 21.7630 - val_mean_absolute_error: 0.0434\n",
            "Epoch 6/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 20.6464 - mean_absolute_error: 0.0379\n",
            "Epoch 00006: val_loss did not improve\n",
            "120/120 [==============================] - 251s 2s/step - loss: 20.6447 - mean_absolute_error: 0.0379 - val_loss: 22.1364 - val_mean_absolute_error: 0.0435\n",
            "Epoch 00006: early stopping\n",
            "Checkpoint. Fold 4 | Iteration 2 (LR 0.000200) -- Score --> 0.04289718 | 21.57374738 \n",
            "Score Better than before (21.57374738 | 21.67569453)\n",
            "Reducing LR to 0.000040\n",
            "Epoch 1/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 20.7480 - mean_absolute_error: 0.0383\n",
            "Epoch 00001: val_loss improved from inf to 21.65968, saving model to weights/cfm-example-model-2018-07-05-best-fold-4-it-3.h5\n",
            "120/120 [==============================] - 257s 2s/step - loss: 20.7478 - mean_absolute_error: 0.0383 - val_loss: 21.6597 - val_mean_absolute_error: 0.0433\n",
            "Epoch 2/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 20.7353 - mean_absolute_error: 0.0381\n",
            "Epoch 00002: val_loss improved from 21.65968 to 21.58419, saving model to weights/cfm-example-model-2018-07-05-best-fold-4-it-3.h5\n",
            "120/120 [==============================] - 253s 2s/step - loss: 20.7324 - mean_absolute_error: 0.0381 - val_loss: 21.5842 - val_mean_absolute_error: 0.0427\n",
            "Epoch 3/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 20.7028 - mean_absolute_error: 0.0380\n",
            "Epoch 00003: val_loss did not improve\n",
            "120/120 [==============================] - 252s 2s/step - loss: 20.7025 - mean_absolute_error: 0.0380 - val_loss: 21.6103 - val_mean_absolute_error: 0.0430\n",
            "Epoch 4/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 20.6207 - mean_absolute_error: 0.0379\n",
            "Epoch 00004: val_loss did not improve\n",
            "120/120 [==============================] - 251s 2s/step - loss: 20.6187 - mean_absolute_error: 0.0379 - val_loss: 21.7211 - val_mean_absolute_error: 0.0438\n",
            "Epoch 5/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 20.5480 - mean_absolute_error: 0.0379\n",
            "Epoch 00005: val_loss did not improve\n",
            "120/120 [==============================] - 254s 2s/step - loss: 20.5480 - mean_absolute_error: 0.0379 - val_loss: 21.6730 - val_mean_absolute_error: 0.0433\n",
            "Epoch 00005: early stopping\n",
            "Checkpoint. Fold 4 | Iteration 3 (LR 0.000040) -- Score --> 0.04271309 | 21.58418821 \n",
            "Score Worse than before (21.58418821 | 21.57374738)\n",
            "Try one last time reducing LR.\n",
            "Epoch 1/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 20.8266 - mean_absolute_error: 0.0384\n",
            "Epoch 00001: val_loss improved from inf to 21.55853, saving model to weights/cfm-example-model-2018-07-05-best-fold-4-it-4.h5\n",
            "120/120 [==============================] - 261s 2s/step - loss: 20.8239 - mean_absolute_error: 0.0384 - val_loss: 21.5585 - val_mean_absolute_error: 0.0429\n",
            "Epoch 2/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 20.8086 - mean_absolute_error: 0.0383\n",
            "Epoch 00002: val_loss did not improve\n",
            "120/120 [==============================] - 251s 2s/step - loss: 20.8108 - mean_absolute_error: 0.0383 - val_loss: 21.5791 - val_mean_absolute_error: 0.0432\n",
            "Epoch 3/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 20.7541 - mean_absolute_error: 0.0383\n",
            "Epoch 00003: val_loss did not improve\n",
            "120/120 [==============================] - 251s 2s/step - loss: 20.7554 - mean_absolute_error: 0.0383 - val_loss: 21.5842 - val_mean_absolute_error: 0.0432\n",
            "Epoch 4/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 20.7289 - mean_absolute_error: 0.0383\n",
            "Epoch 00004: val_loss did not improve\n",
            "120/120 [==============================] - 251s 2s/step - loss: 20.7303 - mean_absolute_error: 0.0383 - val_loss: 21.5805 - val_mean_absolute_error: 0.0432\n",
            "Epoch 00004: early stopping\n",
            "Checkpoint. Fold 4 | Iteration 4 (LR 0.000004) -- Score --> 0.04288695 | 21.55853424 \n",
            "Score Better than before (21.55853424 | 21.57374738)\n",
            "Reducing LR to 0.000001\n",
            "Stop. Too Much Iterations.\n",
            "End of Training for Fold 4. Predicting.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "End of Fold 4 | Score --> 21.5585 | 0.0429\n",
            "Train 509051 , Valid 127262\n",
            "Start Fold 5 - start base_lr 0.0010 : \n",
            "Epoch 1/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 30.9816 - mean_absolute_error: 0.0540\n",
            "Epoch 00001: val_loss improved from inf to 23.27128, saving model to weights/cfm-example-model-2018-07-05-best-fold-5-it-1.h5\n",
            "120/120 [==============================] - 263s 2s/step - loss: 30.9233 - mean_absolute_error: 0.0540 - val_loss: 23.2713 - val_mean_absolute_error: 0.0465\n",
            "Epoch 2/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 22.2826 - mean_absolute_error: 0.0423\n",
            "Epoch 00002: val_loss improved from 23.27128 to 21.84358, saving model to weights/cfm-example-model-2018-07-05-best-fold-5-it-1.h5\n",
            "120/120 [==============================] - 254s 2s/step - loss: 22.2774 - mean_absolute_error: 0.0422 - val_loss: 21.8436 - val_mean_absolute_error: 0.0419\n",
            "Epoch 3/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 22.1326 - mean_absolute_error: 0.0416\n",
            "Epoch 00003: val_loss improved from 21.84358 to 21.75110, saving model to weights/cfm-example-model-2018-07-05-best-fold-5-it-1.h5\n",
            "120/120 [==============================] - 253s 2s/step - loss: 22.1315 - mean_absolute_error: 0.0416 - val_loss: 21.7511 - val_mean_absolute_error: 0.0413\n",
            "Epoch 4/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 21.5707 - mean_absolute_error: 0.0406\n",
            "Epoch 00004: val_loss improved from 21.75110 to 21.74925, saving model to weights/cfm-example-model-2018-07-05-best-fold-5-it-1.h5\n",
            "120/120 [==============================] - 257s 2s/step - loss: 21.5726 - mean_absolute_error: 0.0406 - val_loss: 21.7493 - val_mean_absolute_error: 0.0419\n",
            "Epoch 5/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 21.6601 - mean_absolute_error: 0.0406\n",
            "Epoch 00005: val_loss improved from 21.74925 to 21.55946, saving model to weights/cfm-example-model-2018-07-05-best-fold-5-it-1.h5\n",
            "120/120 [==============================] - 254s 2s/step - loss: 21.6555 - mean_absolute_error: 0.0406 - val_loss: 21.5595 - val_mean_absolute_error: 0.0405\n",
            "Epoch 6/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 21.4383 - mean_absolute_error: 0.0402\n",
            "Epoch 00006: val_loss did not improve\n",
            "120/120 [==============================] - 252s 2s/step - loss: 21.4370 - mean_absolute_error: 0.0402 - val_loss: 22.0062 - val_mean_absolute_error: 0.0420\n",
            "Epoch 7/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 21.2775 - mean_absolute_error: 0.0398\n",
            "Epoch 00007: val_loss did not improve\n",
            "120/120 [==============================] - 251s 2s/step - loss: 21.2754 - mean_absolute_error: 0.0398 - val_loss: 21.6224 - val_mean_absolute_error: 0.0412\n",
            "Epoch 8/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 21.2331 - mean_absolute_error: 0.0398\n",
            "Epoch 00008: val_loss did not improve\n",
            "120/120 [==============================] - 252s 2s/step - loss: 21.2317 - mean_absolute_error: 0.0397 - val_loss: 21.8248 - val_mean_absolute_error: 0.0407\n",
            "Epoch 00008: early stopping\n",
            "Checkpoint. Fold 5 | Iteration 1 (LR 0.001000) -- Score --> 0.04050426 | 21.55945625 \n",
            "Score Better than before (21.55945625 | 100.00000000)\n",
            "Reducing LR to 0.000200\n",
            "Epoch 1/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 21.2279 - mean_absolute_error: 0.0400\n",
            "Epoch 00001: val_loss improved from inf to 21.57289, saving model to weights/cfm-example-model-2018-07-05-best-fold-5-it-2.h5\n",
            "120/120 [==============================] - 262s 2s/step - loss: 21.2221 - mean_absolute_error: 0.0400 - val_loss: 21.5729 - val_mean_absolute_error: 0.0402\n",
            "Epoch 2/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 20.9683 - mean_absolute_error: 0.0394\n",
            "Epoch 00002: val_loss did not improve\n",
            "120/120 [==============================] - 252s 2s/step - loss: 20.9675 - mean_absolute_error: 0.0394 - val_loss: 21.6630 - val_mean_absolute_error: 0.0409\n",
            "Epoch 3/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 20.9390 - mean_absolute_error: 0.0392\n",
            "Epoch 00003: val_loss did not improve\n",
            "120/120 [==============================] - 252s 2s/step - loss: 20.9407 - mean_absolute_error: 0.0392 - val_loss: 21.6099 - val_mean_absolute_error: 0.0407\n",
            "Epoch 4/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 20.7507 - mean_absolute_error: 0.0387\n",
            "Epoch 00004: val_loss did not improve\n",
            "120/120 [==============================] - 254s 2s/step - loss: 20.7526 - mean_absolute_error: 0.0387 - val_loss: 21.7323 - val_mean_absolute_error: 0.0406\n",
            "Epoch 00004: early stopping\n",
            "Checkpoint. Fold 5 | Iteration 2 (LR 0.000200) -- Score --> 0.04015330 | 21.57288558 \n",
            "Score Worse than before (21.57288558 | 21.55945625)\n",
            "Try one last time reducing LR.\n",
            "Epoch 1/50\n",
            "119/120 [============================>.] - ETA: 2s - loss: 21.2298 - mean_absolute_error: 0.0399\n",
            "Epoch 00001: val_loss improved from inf to 21.49624, saving model to weights/cfm-example-model-2018-07-05-best-fold-5-it-3.h5\n",
            "120/120 [==============================] - 263s 2s/step - loss: 21.2287 - mean_absolute_error: 0.0399 - val_loss: 21.4962 - val_mean_absolute_error: 0.0406\n",
            "Epoch 2/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 21.1214 - mean_absolute_error: 0.0397\n",
            "Epoch 00002: val_loss improved from 21.49624 to 21.48734, saving model to weights/cfm-example-model-2018-07-05-best-fold-5-it-3.h5\n",
            "120/120 [==============================] - 257s 2s/step - loss: 21.1211 - mean_absolute_error: 0.0397 - val_loss: 21.4873 - val_mean_absolute_error: 0.0408\n",
            "Epoch 3/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 21.0441 - mean_absolute_error: 0.0397\n",
            "Epoch 00003: val_loss improved from 21.48734 to 21.47723, saving model to weights/cfm-example-model-2018-07-05-best-fold-5-it-3.h5\n",
            "120/120 [==============================] - 254s 2s/step - loss: 21.0451 - mean_absolute_error: 0.0397 - val_loss: 21.4772 - val_mean_absolute_error: 0.0407\n",
            "Epoch 4/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 20.9826 - mean_absolute_error: 0.0396\n",
            "Epoch 00004: val_loss did not improve\n",
            "120/120 [==============================] - 251s 2s/step - loss: 20.9859 - mean_absolute_error: 0.0396 - val_loss: 21.4971 - val_mean_absolute_error: 0.0406\n",
            "Epoch 5/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 20.9841 - mean_absolute_error: 0.0395\n",
            "Epoch 00005: val_loss improved from 21.47723 to 21.47334, saving model to weights/cfm-example-model-2018-07-05-best-fold-5-it-3.h5\n",
            "120/120 [==============================] - 252s 2s/step - loss: 20.9850 - mean_absolute_error: 0.0395 - val_loss: 21.4733 - val_mean_absolute_error: 0.0407\n",
            "Epoch 6/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 20.9498 - mean_absolute_error: 0.0395\n",
            "Epoch 00006: val_loss did not improve\n",
            "120/120 [==============================] - 253s 2s/step - loss: 20.9484 - mean_absolute_error: 0.0395 - val_loss: 21.4929 - val_mean_absolute_error: 0.0408\n",
            "Epoch 7/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 20.8999 - mean_absolute_error: 0.0394\n",
            "Epoch 00007: val_loss did not improve\n",
            "120/120 [==============================] - 252s 2s/step - loss: 20.8984 - mean_absolute_error: 0.0394 - val_loss: 21.4806 - val_mean_absolute_error: 0.0407\n",
            "Epoch 8/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 20.9264 - mean_absolute_error: 0.0393\n",
            "Epoch 00008: val_loss did not improve\n",
            "120/120 [==============================] - 254s 2s/step - loss: 20.9239 - mean_absolute_error: 0.0393 - val_loss: 21.4877 - val_mean_absolute_error: 0.0408\n",
            "Epoch 00008: early stopping\n",
            "Checkpoint. Fold 5 | Iteration 3 (LR 0.000020) -- Score --> 0.04074244 | 21.47333851 \n",
            "Score Better than before (21.47333851 | 21.55945625)\n",
            "Reducing LR to 0.000004\n",
            "Epoch 1/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 20.9277 - mean_absolute_error: 0.0395\n",
            "Epoch 00001: val_loss improved from inf to 21.46868, saving model to weights/cfm-example-model-2018-07-05-best-fold-5-it-4.h5\n",
            "120/120 [==============================] - 256s 2s/step - loss: 20.9254 - mean_absolute_error: 0.0395 - val_loss: 21.4687 - val_mean_absolute_error: 0.0406\n",
            "Epoch 2/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 20.9250 - mean_absolute_error: 0.0394\n",
            "Epoch 00002: val_loss did not improve\n",
            "120/120 [==============================] - 249s 2s/step - loss: 20.9208 - mean_absolute_error: 0.0394 - val_loss: 21.4904 - val_mean_absolute_error: 0.0406\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 20.9196 - mean_absolute_error: 0.0394\n",
            "Epoch 00003: val_loss did not improve\n",
            "120/120 [==============================] - 253s 2s/step - loss: 20.9155 - mean_absolute_error: 0.0394 - val_loss: 21.4957 - val_mean_absolute_error: 0.0406\n",
            "Epoch 4/50\n",
            "119/120 [============================>.] - ETA: 1s - loss: 20.9616 - mean_absolute_error: 0.0394\n",
            "Epoch 00004: val_loss did not improve\n",
            "120/120 [==============================] - 227s 2s/step - loss: 20.9607 - mean_absolute_error: 0.0394 - val_loss: 21.4812 - val_mean_absolute_error: 0.0407\n",
            "Epoch 00004: early stopping\n",
            "Checkpoint. Fold 5 | Iteration 4 (LR 0.000004) -- Score --> 0.04064205 | 21.46868292 \n",
            "Score Better than before (21.46868292 | 21.47333851)\n",
            "Reducing LR to 0.000001\n",
            "Stop. Too Much Iterations.\n",
            "End of Training for Fold 5. Predicting.\n",
            "End of Fold 5 | Score --> 21.4687 | 0.0406\n"
          ]
        }
      ],
      "execution_count": 19,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "cvscores"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 20,
          "data": {
            "text/plain": [
              "[(0.04151054384807746, 21.57486012776693),\n",
              " (0.03769589588046074, 21.247226905822753),\n",
              " (0.03940941691398621, 20.93321024576823),\n",
              " (0.042886945853630704, 21.558534240722658),\n",
              " (0.04064204879105091, 21.468682924906414)]"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 20,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "mean_cv_score = np.mean(cvscores, axis=0)\n",
        "mean_cv_score"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 21,
          "data": {
            "text/plain": [
              "array([ 0.04042897, 21.35650289])"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 21,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "avg_test_score = np.mean(test_preds, axis=0)\n",
        "out_name = 'results/%s-%d-Kfolds_%0.6f_%0.6f.csv' % (model_prefix, total_folds, mean_cv_score[0], mean_cv_score[1])\n",
        "submission = pd.DataFrame({'ID':test_df[\"ID\"], 'TARGET':avg_test_score.ravel()})\n",
        "submission.to_csv(out_name , index=False)"
      ],
      "outputs": [],
      "execution_count": 22,
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda root]",
      "language": "python",
      "name": "conda-root-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}